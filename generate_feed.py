import feedparser
from feedgen.feed import FeedGenerator
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import json

# ==============================
# üîó Flux RSS OVHcloud √† agr√©ger
# ==============================
SOURCES = [
    "https://www.ovhcloud.com/fr/blog/feed/",
    "https://press.ovhcloud.com/feed/",
    "https://www.ovhcloud.com/fr/blog/tag/telecom/feed/",
    "https://www.ovh.com/fr/blog/tag/security/feed/"
]

# ==============================
# ‚öôÔ∏è Fonctions utilitaires
# ==============================
def clean_html(text: str) -> str:
    """Nettoie le HTML pour ne garder que le texte brut."""
    return BeautifulSoup(text or "", "html.parser").get_text()

def safe_parse(url: str):
    """R√©cup√®re un flux RSS en toute s√©curit√©, ignore 404 ou erreurs."""
    try:
        print(f"üîÑ Tentative de r√©cup√©ration du flux : {url}")
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            print(f"‚ö†Ô∏è Flux inaccessible ({response.status_code}) : {url}")
            return None
        feed = feedparser.parse(response.text)
        if not feed.entries:
            print(f"‚ö†Ô∏è Aucun article trouv√© dans {url}")
        return feed
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur r√©seau sur {url} : {e}")
        return None

# ==============================
# üß© Cr√©ation du flux RSS fusionn√©
# ==============================
fg = FeedGenerator()
fg.title("Actualit√©s OVHcloud (blog + presse + t√©l√©com)")
fg.link(href="https://www.ovhcloud.com/fr/", rel="alternate")
fg.description(f"Flux RSS regroupant toutes les actualit√©s OVHcloud ‚Äî mis √† jour le {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}")
fg.language("fr")

entries = []
failed_sources = []

for url in SOURCES:
    feed = safe_parse(url)
    if feed:
        entries.extend(feed.entries)
    else:
        failed_sources.append(url)

# Tri par date d√©croissante
entries.sort(key=lambda e: getattr(e, "published_parsed", None) or datetime.utcnow(), reverse=True)

# Ajout au flux RSS
for entry in entries:
    fe = fg.add_entry()
    fe.title(entry.title)
    fe.link(href=entry.link)
    fe.description(clean_html(getattr(entry, "summary", "")))
    fe.published(getattr(entry, "published", datetime.utcnow().isoformat()))

# G√©n√©ration du fichier RSS
fg.rss_file("rss.xml")
print("‚úÖ Flux RSS g√©n√©r√© : rss.xml")

# ==============================
# üíæ G√©n√©ration du fichier JSON
# ==============================
json_feed = {
    "title": "Actualit√©s OVHcloud (blog + presse + t√©l√©com)",
    "updated": datetime.utcnow().isoformat(),
    "source_count": len(SOURCES),
    "failed_sources": failed_sources,
    "entries": []
}

for entry in entries:
    json_feed["entries"].append({
        "title": entry.title,
        "link": entry.link,
        "published": getattr(entry, "published", ""),
        "summary": clean_html(getattr(entry, "summary", ""))
    })

with open("feed.json", "w", encoding="utf-8") as f:
    json.dump(json_feed, f, ensure_ascii=False, indent=2)

print("‚úÖ Flux JSON g√©n√©r√© : feed.json")

# üîπ Message r√©sum√© des flux HS
if failed_sources:
    print(f"‚ö†Ô∏è Flux HS d√©tect√©s ({len(failed_sources)}):")
    for f in failed_sources:
        print(f" - {f}")
else:
    print("‚úÖ Tous les flux ont √©t√© r√©cup√©r√©s avec succ√®s")

